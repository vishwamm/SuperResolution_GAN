{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Conv2D, PReLU,BatchNormalization, Flatten\n",
    "from keras.layers import UpSampling2D, LeakyReLU, Dense, Input, add\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "\n",
    "#Define blocks to build the generator\n",
    "def res_block(ip):\n",
    "    \n",
    "    res_model = Conv2D(64, (3,3), padding = \"same\")(ip)\n",
    "    res_model = BatchNormalization(momentum = 0.5)(res_model)\n",
    "    res_model = PReLU(shared_axes = [1,2])(res_model)\n",
    "    \n",
    "    res_model = Conv2D(64, (3,3), padding = \"same\")(res_model)\n",
    "    res_model = BatchNormalization(momentum = 0.5)(res_model)\n",
    "    \n",
    "    return add([ip,res_model])\n",
    "\n",
    "def upscale_block(ip):\n",
    "    \n",
    "    up_model = Conv2D(256, (3,3), padding=\"same\")(ip)\n",
    "    up_model = UpSampling2D( size = 2 )(up_model)\n",
    "    up_model = PReLU(shared_axes=[1,2])(up_model)\n",
    "    \n",
    "    return up_model\n",
    "\n",
    "#Generator model\n",
    "def create_gen(gen_ip, num_res_block):\n",
    "    layers = Conv2D(64, (9,9), padding=\"same\")(gen_ip)\n",
    "    layers = PReLU(shared_axes=[1,2])(layers)\n",
    "\n",
    "    temp = layers\n",
    "\n",
    "    for i in range(num_res_block):\n",
    "        layers = res_block(layers)\n",
    "\n",
    "    layers = Conv2D(64, (3,3), padding=\"same\")(layers)\n",
    "    layers = BatchNormalization(momentum=0.5)(layers)\n",
    "    layers = add([layers,temp])\n",
    "\n",
    "    layers = upscale_block(layers)\n",
    "    layers = upscale_block(layers)\n",
    "\n",
    "    op = Conv2D(3, (9,9), padding=\"same\")(layers)\n",
    "\n",
    "    return Model(inputs=gen_ip, outputs=op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriminator block that will be used to construct the discriminator\n",
    "def discriminator_block(ip, filters, strides=1, bn=True):\n",
    "    \n",
    "    disc_model = Conv2D(filters, (3,3), strides = strides, padding=\"same\")(ip)\n",
    "    \n",
    "    if bn:\n",
    "        disc_model = BatchNormalization( momentum=0.8 )(disc_model)\n",
    "    \n",
    "    disc_model = LeakyReLU( alpha=0.2 )(disc_model)\n",
    "    \n",
    "    return disc_model\n",
    "\n",
    "\n",
    "#Descriminartor, as described in the original paper\n",
    "def create_disc(disc_ip):\n",
    "\n",
    "    df = 64\n",
    "    \n",
    "    d1 = discriminator_block(disc_ip, df, bn=False)\n",
    "    d2 = discriminator_block(d1, df, strides=2)\n",
    "    d3 = discriminator_block(d2, df*2)\n",
    "    d4 = discriminator_block(d3, df*2, strides=2)\n",
    "    d5 = discriminator_block(d4, df*4)\n",
    "    d6 = discriminator_block(d5, df*4, strides=2)\n",
    "    d7 = discriminator_block(d6, df*8)\n",
    "    d8 = discriminator_block(d7, df*8, strides=2)\n",
    "    \n",
    "    d8_5 = Flatten()(d8)\n",
    "    d9 = Dense(df*16)(d8_5)\n",
    "    d10 = LeakyReLU(alpha=0.2)(d9)\n",
    "    validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "    return Model(disc_ip, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG19 \n",
    "#We need VGG19 for the feature map obtained by the j-th convolution (after activation) \n",
    "#before the i-th maxpooling layer within the VGG19 network.(as described in the paper)\n",
    "#Let us pick the 3rd block, last conv layer. \n",
    "#Build a pre-trained VGG19 model that outputs image features extracted at the\n",
    "# third block of the model\n",
    "# VGG architecture: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "from keras.applications import VGG19\n",
    "\n",
    "def build_vgg(hr_shape):\n",
    "    \n",
    "    vgg = VGG19(weights=\"imagenet\",include_top=False, input_shape=hr_shape)\n",
    "    \n",
    "    return Model(inputs=vgg.inputs, outputs=vgg.layers[10].output)\n",
    "\n",
    "#Combined model\n",
    "def create_comb(gen_model, disc_model, vgg, lr_ip, hr_ip):\n",
    "    gen_img = gen_model(lr_ip)\n",
    "    \n",
    "    gen_features = vgg(gen_img)\n",
    "    \n",
    "    disc_model.trainable = False\n",
    "    validity = disc_model(gen_img)\n",
    "    \n",
    "    return Model(inputs=[lr_ip, hr_ip], outputs=[validity, gen_features])\n",
    "\n",
    "# 2 losses... adversarial loss and content (VGG) loss\n",
    "#AdversariaL: is defined based on the probabilities of the discriminator over all training samples\n",
    "# use binary_crossentropy\n",
    "\n",
    "#Content: feature map obtained by the j-th convolution (after activation) \n",
    "#before the i-th maxpooling layer within the VGG19 network.\n",
    "# MSE between the feature representations of a reconstructed image\n",
    "# and the reference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "\n",
    "#Load first n number of images (to train on a subset of all images)\n",
    "#For demo purposes, let us use 5000 images\n",
    "n=5000\n",
    "lr_list = os.listdir(\"dataset/lr_images\")[:n]\n",
    "\n",
    "lr_images = []\n",
    "for img in lr_list:\n",
    "    img_lr = cv2.imread(\"dataset/lr_images/\" + img)\n",
    "    img_lr = cv2.cvtColor(img_lr, cv2.COLOR_BGR2RGB)\n",
    "    lr_images.append(img_lr)   \n",
    "\n",
    "\n",
    "hr_list = os.listdir(\"dataset/hr_images\")[:n]\n",
    "   \n",
    "hr_images = []\n",
    "for img in hr_list:\n",
    "    img_hr = cv2.imread(\"dataset/hr_images/\" + img)\n",
    "    img_hr = cv2.cvtColor(img_hr, cv2.COLOR_BGR2RGB)\n",
    "    hr_images.append(img_hr)   \n",
    "\n",
    "lr_images = np.array(lr_images)\n",
    "hr_images = np.array(hr_images)\n",
    "\n",
    "#Sanity check, view few mages\n",
    "import random\n",
    "import numpy as np\n",
    "image_number = random.randint(0, len(lr_images)-1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.reshape(lr_images[image_number], (32, 32, 3)))\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.reshape(hr_images[image_number], (128, 128, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale values\n",
    "lr_images = lr_images / 255.\n",
    "hr_images = hr_images / 255.\n",
    "\n",
    "#Split to train and test\n",
    "lr_train, lr_test, hr_train, hr_test = train_test_split(lr_images, hr_images, \n",
    "                                                      test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "hr_shape = (hr_train.shape[1], hr_train.shape[2], hr_train.shape[3])\n",
    "lr_shape = (lr_train.shape[1], lr_train.shape[2], lr_train.shape[3])\n",
    "\n",
    "lr_ip = Input(shape=lr_shape)\n",
    "hr_ip = Input(shape=hr_shape)\n",
    "\n",
    "generator = create_gen(lr_ip, num_res_block = 16)\n",
    "generator.summary()\n",
    "\n",
    "discriminator = create_disc(hr_ip)\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "discriminator.summary()\n",
    "\n",
    "vgg = build_vgg((128,128,3))\n",
    "print(vgg.summary())\n",
    "vgg.trainable = False\n",
    "\n",
    "gan_model = create_comb(generator, discriminator, vgg, lr_ip, hr_ip)\n",
    "\n",
    "# 2 losses... adversarial loss and content (VGG) loss\n",
    "#AdversariaL: is defined based on the probabilities of the discriminator over all training samples\n",
    "# use binary_crossentropy\n",
    "\n",
    "#Content: feature map obtained by the j-th convolution (after activation) \n",
    "#before the i-th maxpooling layer within the VGG19 network.\n",
    "# MSE between the feature representations of a reconstructed image\n",
    "# and the reference image. \n",
    "gan_model.compile(loss=[\"binary_crossentropy\", \"mse\"], loss_weights=[1e-3, 1], optimizer=\"adam\")\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of images for LR and HR in batches from which a batch of images\n",
    "#would be fetched during training. \n",
    "batch_size = 1  \n",
    "train_lr_batches = []\n",
    "train_hr_batches = []\n",
    "for it in range(int(hr_train.shape[0] / batch_size)):\n",
    "    start_idx = it * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    train_hr_batches.append(hr_train[start_idx:end_idx])\n",
    "    train_lr_batches.append(lr_train[start_idx:end_idx])\n",
    "    \n",
    "    \n",
    "epochs = 5\n",
    "#Enumerate training over epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    fake_label = np.zeros((batch_size, 1)) # Assign a label of 0 to all fake (generated images)\n",
    "    real_label = np.ones((batch_size,1)) # Assign a label of 1 to all real images.\n",
    "    \n",
    "    #Create empty lists to populate gen and disc losses. \n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    #Enumerate training over batches. \n",
    "    for b in tqdm(range(len(train_hr_batches))):\n",
    "        lr_imgs = train_lr_batches[b] #Fetch a batch of LR images for training\n",
    "        hr_imgs = train_hr_batches[b] #Fetch a batch of HR images for training\n",
    "        \n",
    "        fake_imgs = generator.predict_on_batch(lr_imgs) #Fake images\n",
    "        \n",
    "        #First, train the discriminator on fake and real HR images. \n",
    "        discriminator.trainable = True\n",
    "        d_loss_gen = discriminator.train_on_batch(fake_imgs, fake_label)\n",
    "        d_loss_real = discriminator.train_on_batch(hr_imgs, real_label)\n",
    "        \n",
    "        #Now, train the generator by fixing discriminator as non-trainable\n",
    "        discriminator.trainable = False\n",
    "        \n",
    "        #Average the discriminator loss, just for reporting purposes. \n",
    "        d_loss = 0.5 * np.add(d_loss_gen, d_loss_real) \n",
    "        \n",
    "        #Extract VGG features, to be used towards calculating loss\n",
    "        image_features = vgg.predict(hr_imgs)\n",
    "     \n",
    "        #Train the generator via GAN. \n",
    "        #Remember that we have 2 losses, adversarial loss and content (VGG) loss\n",
    "        g_loss, _, _ = gan_model.train_on_batch([lr_imgs, hr_imgs], [real_label, image_features])\n",
    "        \n",
    "        #Save losses to a list so we can average and report. \n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "    #Convert the list of losses to an array to make it easy to average    \n",
    "    g_losses = np.array(g_losses)\n",
    "    d_losses = np.array(d_losses)\n",
    "    \n",
    "    #Calculate the average losses for generator and discriminator\n",
    "    g_loss = np.sum(g_losses, axis=0) / len(g_losses)\n",
    "    d_loss = np.sum(d_losses, axis=0) / len(d_losses)\n",
    "    \n",
    "    #Report the progress during training. \n",
    "    print(\"epoch:\", e+1 ,\"g_loss:\", g_loss, \"d_loss:\", d_loss)\n",
    "\n",
    "    if (e+1) % 10 == 0: #Change the frequency for model saving, if needed\n",
    "        #Save the generator after every n epochs (Usually 10 epochs)\n",
    "        generator.save(\"gen_e_\"+ str(e+1) +\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "#Test - perform super resolution using saved generator model\n",
    "from keras.models import load_model\n",
    "from numpy.random import randint\n",
    "\n",
    "generator = load_model('gen_e_10.h5', compile=False)\n",
    "\n",
    "\n",
    "[X1, X2] = [lr_test, hr_test]\n",
    "# select random example\n",
    "ix = randint(0, len(X1), 1)\n",
    "src_image, tar_image = X1[ix], X2[ix]\n",
    "\n",
    "# generate image from source\n",
    "gen_image = generator.predict(src_image)\n",
    "\n",
    "\n",
    "# plot all three images\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('LR Image')\n",
    "plt.imshow(src_image[0,:,:,:])\n",
    "plt.subplot(232)\n",
    "plt.title('Superresolution')\n",
    "plt.imshow(gen_image[0,:,:,:])\n",
    "plt.subplot(233)\n",
    "plt.title('Orig. HR image')\n",
    "plt.imshow(tar_image[0,:,:,:])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GANs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
